# -*- coding: utf-8 -*-
"""Netflix Recommendation System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xHdJrhSoY9AkR9XL5LQM23CBZyDRxPvy
"""

#import modules
!pip install Fuzzywuzzy

import numpy as np
import pandas as pd
from difflib import SequenceMatcher
from fuzzywuzzy import fuzz
from sklearn.metrics.pairwise import linear_kernel, cosine_similarity, pairwise_distances, manhattan_distances,euclidean_distances
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
import csv
import io
import random

import requests
import csv

# Download the CSV file from GitHub
url = 'https://raw.githubusercontent.com/ykwong1223/Principle-DS-new/main/Netflix.csv'
response = requests.get(url)
content = response.content.decode('utf-8')
csvreader = csv.reader(content.splitlines())
raw = pd.read_csv(io.StringIO(content))

#Import Dataset

raw=pd.DataFrame(raw)

#Remove duplicate and check for null
print(raw.shape)
raw.drop_duplicates(keep='last', inplace=True)
missing=raw.isnull().sum()
print(missing)

#Replace null and check again


for i in ["country","date_added","rating"]:
  raw[i].fillna(raw[i].mode().iloc[0], inplace=True)

for i in ["director","cast"]:
  raw[i].fillna("", inplace=True)

missing=raw.isnull().sum()
print(missing)

#remove spcae and lower case the words and combine
def data_clean(k):
  return str.lower(k.replace(" ",""))
raw["combine"]=raw["title"].apply(data_clean)+" "+raw["cast"].apply(data_clean)+" "+raw["director"].apply(data_clean)+" "+raw["description"].apply(data_clean)

#Using Sequence Matcher to find similarity of description and choose the 10 highest similarity contents

def seq_similar(nm):
    j = raw[raw['title'] == nm].index[0]
    string1 = str(raw.loc[j, "combine"])
    for i in range(len(raw)):
      string2 = str(raw.loc[i, "combine"])
      sequence_ratio=SequenceMatcher(None, string1, string2).ratio()
      raw.at[i, "seq_ratio"] = sequence_ratio
    subset=raw[["title","seq_ratio"]]
    subset=subset.sort_values(by='seq_ratio',ascending=False)
    return subset

seq_similar("Naruto")[1:11]

#Using Fuzzywuzzy to find similarity of description and choose the 10 highest similarity contents

def fuz_ratio(nm):
    j = raw[raw['title'] == nm].index[0]
    string1 = str(raw.loc[j, "combine"])
    for i in range(len(raw)):
      string2 = str(raw.loc[i, "combine"])
      # Calculate similarity using the ratio method (Levenshtein distance)
      similarity_ratio = fuzz.ratio(string1, string2)
      raw.at[i, "fuz_ratio"] = similarity_ratio
    subset=raw[["title","fuz_ratio"]]
    subset=subset.sort_values(by='fuz_ratio',ascending=False)
    return subset

fuz_ratio("Naruto")[1:11]

#Using Dice Coefficient to find similarity of description and choose the 10 highest similarity contents
def dice_coefficient(nm):
    j = raw[raw['title'] == nm].index[0]
    string1 = str(raw.loc[j, "combine"])
    for i in range(len(raw)):
      if i!=j:
        string2 = str(raw.loc[i, "combine"])
        string1=set(string1)
        string2=set(string2)
        intersection_size = len(string1.intersection(string2))
        dice_coefficient = (2 * intersection_size) / (len(string1) + len(string2))
      else:
        dice_coefficient=1.1
      raw.at[i, "dice_coefficient"] = dice_coefficient
    subset=raw[["title","dice_coefficient"]]
    subset=subset.sort_values(by='dice_coefficient',ascending=False)
    return subset


dice_coefficient("Naruto")[1:11]

#Using Jaccard Coefficient to find similarity of description and choose the 10 highest similarity contents
def jaccard_coefficient(nm):
    j = raw[raw['title'] == nm].index[0]
    string1 = str(raw.loc[j, "combine"])
    for i in range(len(raw)):
      if i!=j:
        string2 = str(raw.loc[i, "combine"])
        string1 = set(string1)
        string2 = set(string2)
        intersection_size = len(string1.intersection(string2))
        union_size = len(string1.union(string2))
        jaccard_coefficient = intersection_size / union_size
      else:
        jaccard_coefficient=1.1
      raw.at[i, "jaccard_coefficient"] = jaccard_coefficient
    subset=raw[["title","jaccard_coefficient"]]
    subset=subset.sort_values(by="jaccard_coefficient",ascending=False)
    return subset

jaccard_coefficient("Naruto")[1:11]

#Vectorize "combine" column to matrx and use cosine similarity to find similarity of description and choose the 10 highest similarity contents
count = CountVectorizer().fit_transform(raw['combine'])
cosine_similar_score = cosine_similarity(count, count)

def cosine_similar(name):
    indx = raw.index[raw['title'] == name].tolist()
    score=list(enumerate(cosine_similar_score[indx[0]]))
    raw["score"]=[m[1] for m in score]
    subset=raw[["title","score"]]
    subset=subset.sort_values(by='score',ascending=False)
    return subset

cosine_similar("Naruto")[1:11]

# Random select 10 contents
def rnd_model(nm):
    subset=raw[["title"]]
    subset=subset.sample(frac=1)
    return subset


rnd_model("Naruto")[1:11]

#Build a data fram and use euclidean distance and manhattan distance to evaluate the models

performance_df = pd.DataFrame(columns=['Model', 'Total Euclidean Distance', 'Total Manhattan Distance'], index=range(6))
performance_df["Model"] = ["Sequence Matcher", "Fuzzy Ratio", "Dice Coefficient", "Jaccard Coefficient", "Cosine Similarity", "Random"]

vectorizer = TfidfVectorizer()
tfidf_vectors = vectorizer.fit_transform(raw["combine"])
total_euclidean_distance=0

recommendation_models=[seq_similar, fuz_ratio, dice_coefficient, jaccard_coefficient, cosine_similar, rnd_model]

for f in range(6):
  performance_df.loc[f, "Total Euclidean Distance"]=0
  performance_df.loc[f, "Total Manhattan Distance"]=0

for k in range(100):
  random_int = random.randint(0, len(raw)-1)
  name_selected=raw.loc[random_int,"title"]
  for f in range(6):
    mthd=recommendation_models[f]
    subset=mthd(name_selected)
    for g in range(1,11):
        j = raw[raw['title'] == subset.iloc[g]["title"]].index[0]
        manhattan_distance = manhattan_distances(tfidf_vectors[random_int], tfidf_vectors[j])
        euclidean_distance = euclidean_distances(tfidf_vectors[random_int], tfidf_vectors[j])
        performance_df.loc[f, "Total Euclidean Distance"]=euclidean_distance+performance_df.loc[f, "Total Euclidean Distance"]
        performance_df.loc[f, "Total Manhattan Distance"]=manhattan_distance+performance_df.loc[f, "Total Manhattan Distance"]

print(performance_df)

# total_euclidean_distance3=0
# total_manhattan_distance3=0
# for k in range(100):
#   random_int = random.randint(0, len(raw)-1)
#   for g in range(1,11):
#     random_int2=random.randint(0, len(raw)-1)
#     manhattan_distance = manhattan_distances(tfidf_vectors[random_int], tfidf_vectors[random_int2])
#     euclidean_distance = euclidean_distances(tfidf_vectors[random_int], tfidf_vectors[random_int2])
#     total_euclidean_distance3=euclidean_distance+total_euclidean_distance3
#     total_manhattan_distance3=manhattan_distance+total_manhattan_distance3

# print(total_euclidean_distance3)
# print(total_manhattan_distance3)

# #Using Sequence Matcher to find similarity of description and choose the 10 highest similarity contents

# def seq_similar_2(nm):
#     j = raw[raw['title'] == nm].index[0]
#     string1 = str(raw.loc[j, "combine_2"])
#     for i in range(len(raw)):
#       string2 = str(raw.loc[i, "combine_2"])
#       sequence_ratio=SequenceMatcher(None, string1, string2).ratio()
#       raw.at[i, "seq_ratio_2"] = sequence_ratio
#     subset=raw[["title","seq_ratio_2"]]
#     subset=subset.sort_values(by='seq_ratio_2',ascending=False)
#     return subset

# seq_similar_2("Naruto Shippuden the Movie: Blood Prison")[0:10]

# #Using Fuzzywuzzy to find similarity of description and choose the 10 highest similarity contents

# def fuz_ratio_2(nm):
#     j = raw[raw['title'] == nm].index[0]
#     string1 = str(raw.loc[j, "combine_2"])
#     for i in range(len(raw)):
#       string2 = str(raw.loc[i, "combine_2"])
#       # Calculate similarity using the ratio method (Levenshtein distance)
#       similarity_ratio = fuzz.ratio(string1, string2)
#       raw.at[i, "fuz_ratio_2"] = similarity_ratio
#     subset=raw[["title","fuz_ratio_2"]]
#     subset=subset.sort_values(by='fuz_ratio_2',ascending=False)
#     return subset

# fuz_ratio_2("Naruto Shippuden the Movie: Blood Prison")[0:10]

# def dice_coefficient_2(nm):
#     j = raw[raw['title'] == nm].index[0]
#     string1 = str(raw.loc[j, "combine_2"])
#     for i in range(len(raw)):
#       string2 = str(raw.loc[i, "combine_2"])
#       string1=set(string1)
#       string2=set(string2)
#       intersection_size = len(string1.intersection(string2))
#       dice_coefficient = (2 * intersection_size) / (len(string1) + len(string2))
#       raw.at[i, "dice_coefficient_2"] = dice_coefficient
#     subset=raw[["title","dice_coefficient_2"]]
#     subset=subset.sort_values(by='dice_coefficient_2',ascending=False)
#     return subset


# dice_coefficient_2("Welcome")[0:10]

# def jaccard_similar_2(nm):
#     j = raw[raw['title'] == nm].index[0]
#     string1 = str(raw.loc[j, "combine_2"])
#     for i in range(len(raw)):
#       if i!=j:
#         string2 = str(raw.loc[i, "combine_2"])
#         string1 = set(string1)
#         string2 = set(string2)
#         intersection_size = len(string1.intersection(string2))
#         union_size = len(string1.union(string2))
#         jaccard_similarity = intersection_size / union_size
#         raw.at[i, "jaccard_similarity_2"] = jaccard_similarity
#     subset=raw[["title","jaccard_similarity_2"]]
#     subset=subset.sort_values(by='jaccard_similarity_2',ascending=False)
#     return subset

# jaccard_similar_2("Pok√©mon Master Journeys: The Series")[0:10]

# count = CountVectorizer().fit_transform(raw['combine'])
# cosine_similar_score = cosine_similarity(count, count)

# def cosine_similar_2(name):
#     indx = raw.index[raw['title'] == name].tolist()
#     print(indx)
#     score=list(enumerate(cosine_similar_score[indx[0]]))
#     raw["score_2"]=[m[1] for m in score]
#     subset=raw[["title","score_2"]]
#     subset=subset.sort_values(by='score_2',ascending=False)
#     return subset

# cosine_similar_2("Pok√©mon Master Journeys: The Series")[0:10]

# performance_df_2 = pd.DataFrame(columns=['Model', 'Total Euclidean Distance',"Total Manhattan Distance"], index=range(5))
# performance_df_2["Model"] = ["Sequence Similiar 2", "Fuzzy Ratio 2", "Dice Coefficient 2", "Jaccard Similarity 2", "Cosine Similarity 2"]

# vectorizer = TfidfVectorizer()
# tfidf_vectors = vectorizer.fit_transform(raw["combine_2"])

# recommendation_models=[seq_similar_2, fuz_ratio_2, dice_coefficient_2, jaccard_similar_2, cosine_similar_2]
# for f in range(5):
#   total_euclidean_distance=0
#   total_manhattan_distance=0
#   mthd=recommendation_models[f]
#   for k in range(100):
#     random_int = random.randint(0, len(raw)-1)
#     name_selected=raw.loc[random_int,"title"]
#     subset=mthd(name_selected)
#     for g in range(1,11):
#       j = raw[raw['title'] == subset.iloc[g]["title"]].index[0]
#       manhattan_distance = manhattan_distances(tfidf_vectors[random_int], tfidf_vectors[j])
#       euclidean_distance = euclidean_distances(tfidf_vectors[random_int], tfidf_vectors[j])
#       total_euclidean_distance=euclidean_distance+total_euclidean_distance
#       total_manhattan_distance=manhattan_distance+total_manhattan_distance
#   performance_df_2.loc[f, "Total Euclidean Distance"]=total_euclidean_distance
#   performance_df_2.loc[f, "Total Manhattan Distance"]=total_manhattan_distance

# print(performance_df_2)

# import numpy as np
# import pandas as pd
# from sklearn.model_selection import train_test_split
# from sklearn.linear_model import LinearRegression
# from sklearn.metrics import mean_squared_error, r2_score
# # from sklearn.preprocessing import LabelEncoder

# # Load the Netflix movie dataset
# raw = pd.read_csv("Netflix.csv")

# # Handling missing values
# for i in ["country", "date_added", "rating"]:
#     raw[i].fillna(raw[i].mode().iloc[0], inplace=True)

# for i in ["director", "cast"]:
#     raw[i].fillna("", inplace=True)

# # Preprocess 'duration' column
# raw['duration'] = raw['duration'].str.extract('(\d+)')
# raw['duration'] = raw['duration'].astype(float)

# # Select relevant features for prediction (example: duration and release_year)
# features = raw[['duration', 'release_year']]
# target = raw['rating']

# # Handling missing values in features
# features.fillna(features.mean(), inplace=True)

# # Label encode the target variable
# label_encoder = LabelEncoder()
# target_encoded = label_encoder.fit_transform(target)

# # Split the dataset into training and testing sets
# X_train, X_test, y_train, y_test = train_test_split(features, target_encoded, test_size=0.2, random_state=42)

# # Create a linear regression model
# model = LinearRegression()

# # Fit the model to the training data
# model.fit(X_train, y_train)

# # Predict ratings for the test set
# y_pred = model.predict(X_test)

# # Model evaluation
# mse = mean_squared_error(y_test, y_pred)
# r2 = r2_score(y_test, y_pred)

# print("Mean Squared Error:", mse)
# print("R-squared:", r2)

#performance_df = pd.DataFrame(columns=['Model', 'cs simil'], index=range(5))
# performance_df["Model"] = ["Sequence Similiar", "Fuzzy Ratio", "Dice Coefficient", "Jaccard Similarity", "Euclidea Distance"]

# vectorizer = TfidfVectorizer()
# tfidf_vectors = vectorizer.fit_transform(raw["combine"])

# recommendation_models=[seq_similar, fuz_ratio, dice_coefficient, jaccard_similar, euclidean_distance]
# for f in range(5):
#   total_cs_simil=0
#   mthd=recommendation_models[f]
#   for k in range(100):
#     random_int = random.randint(0, len(raw)-1)
#     name_selected=raw.loc[random_int,"title"]
#     subset=mthd(name_selected)
#     for g in range(1,11):
#       j = raw[raw['title'] == subset.iloc[g]["title"]].index[0]
#       cs_simil = cosine_similarity(tfidf_vectors[random_int], tfidf_vectors[j])
#       total_cs_simil=cs_simil+total_cs_simil

#   performance_df.loc[f, "cs simil"]=total_cs_simil

# count = CountVectorizer().fit_transform(raw['combine'])
# euclidean_distance_score = euclidean_distances(count, count)

# def euclidean_distance(name):
#     indx = raw.index[raw['title'] == name].tolist()
#     print(indx)
#     score=list(enumerate(euclidean_distance_score[indx[0]]))
#     raw["score"]=[m[1] for m in score]
#     subset=raw[["title","score"]]
#     subset=subset.sort_values(by='score',ascending=True)
#     return subset

# euclidean_distance("Pok√©mon Master Journeys: The Series")[1:10]